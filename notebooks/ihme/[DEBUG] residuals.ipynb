{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from datetime import datetime, timedelta\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "from matplotlib.dates import DateFormatter\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "import curvefit\n",
    "from curvefit.core.functions import *\n",
    "from curvefit.core.utils import data_translator\n",
    "from curvefit.pipelines.basic_model import *\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_log_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# load params\n",
    "pstr=\"maharashtra\"\n",
    "\n",
    "with open('params.json', \"r\") as paramsfile:\n",
    "    pargs = json.load(paramsfile)\n",
    "    if pstr not in pargs:\n",
    "        print(\"entry not found in params.json\")\n",
    "        sys.exit(0)\n",
    "    pargs = pargs[pstr]\n",
    "\n",
    "\n",
    "# load data\n",
    "data_func = getattr(sys.modules[__name__], pargs['data_func'])\n",
    "if 'data_func_args' in pargs:\n",
    "    df = data_func(pargs['data_func_args'])\n",
    "else:\n",
    "    df = data_func()\n",
    "test_size = pargs['test_size']\n",
    "data, test = df[:-test_size], df[-test_size:]\n",
    "seed = 'last{}'.format(test_size)\n",
    "print ('seed: {}'.format(seed))\n",
    "# data, test = train_test_split(df, train_size=.8, shuffle=True, random_state=seed)\n",
    "\n",
    "# set vars\n",
    "n_data       = len(data)\n",
    "num_params   = 3 # alpha beta p\n",
    "alpha_true   = pargs['alpha_true'] # TODO\n",
    "beta_true    = pargs['beta_true'] # TODO\n",
    "p_true       = pargs['p_true'] # TODO\n",
    "params_true       = np.array( [ alpha_true, beta_true, p_true ] )\n",
    "\n",
    "fname = pstr\n",
    "output_folder = f'output/pipeline/{fname}'\n",
    "if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "date, groupcol = pargs['date'], pargs['groupcol']\n",
    "xcol, ycols = pargs['xcol'], pargs['ycols']\n",
    "for (k,v) in ycols.items():\n",
    "    ycols[k] = getattr(sys.modules[__name__], v)\n",
    "\n",
    "daysforward, daysback = pargs['daysforward'], pargs['daysback']\n",
    "daysback=0\n",
    "pargs['smart_init'] = pargs['smart_init'] if 'smart_init' in pargs else False\n",
    "# link functions\n",
    "identity_fun = lambda x: x\n",
    "exp_fun = lambda x : np.exp(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict_plot(curve_model, xcol, ycol, data, test, func, predictdate, pargs=None):\n",
    "    p_args = {\n",
    "        \"n_draws\": 5,\n",
    "        \"cv_threshold\": 1e-2,\n",
    "        \"smoothed_radius\": [3,3], \n",
    "        \"num_smooths\": 3, \n",
    "        \"exclude_groups\": [], \n",
    "        \"exclude_below\": 0,\n",
    "        \"exp_smoothing\": None, \n",
    "        \"max_last\": None\n",
    "    }\n",
    "    p_args.update(pargs)\n",
    "\n",
    "    predictx = np.array([x+1 for x in range(-daysback,daysforward)])\n",
    "    \n",
    "    # pipeline\n",
    "    pipeline.setup_pipeline()\n",
    "    # TODO: all params for below. the column names for covariates need to be fixed below, and need to be in pipeline.pv.all_residuals\n",
    "    pipeline.run(n_draws=p_args['n_draws'], prediction_times=predictx, \n",
    "        cv_threshold=p_args['cv_threshold'], smoothed_radius=p_args['smoothed_radius'], \n",
    "        num_smooths=p_args['num_smooths'], exclude_groups=p_args['exclude_groups'], \n",
    "        exclude_below=p_args['exclude_below'], exp_smoothing=p_args['exp_smoothing'], \n",
    "        max_last=p_args['max_last']\n",
    "    )\n",
    "    params_estimate = pipeline.mod.params\n",
    "    print(params_estimate)\n",
    "\n",
    "    # plot draws\n",
    "    pipeline.plot_results(prediction_times=predictx)\n",
    "    plt.savefig(f'{output_folder}/draws_{fname}_{ycol}_{func.__name__}_{seed}.png')\n",
    "    plt.clf()\n",
    "    \n",
    "    # for grp in df[groupcol].unique():\n",
    "    predictions = pipeline.predict(times=predictx, predict_space=func, predict_group='all')\n",
    "    \n",
    "    # evaluate against test set\n",
    "    xtest, ytest = test[xcol], test[ycol]\n",
    "    predtest = pipeline.predict(times=xtest, predict_space=func, predict_group='all')\n",
    "    r2, msle = r2_score(ytest, predtest), mean_squared_log_error(ytest, predtest)\n",
    "    print ('test set - r2: {} msle: {}'.format(r2, msle))\n",
    "\n",
    "    # evaluate overall\n",
    "    r2, msle = r2_score(data[ycol], predictions[daysback:daysback+len(data[ycol])]), \\\n",
    "        mean_squared_log_error(data[ycol], predictions[daysback:daysback+len(data[ycol])])\n",
    "    print ('overall - r2: {} msle: {}'.format(r2, msle))\n",
    "\n",
    "\n",
    "    # plot predictions against actual\n",
    "    register_matplotlib_converters()\n",
    "    plt.yscale(\"log\")\n",
    "    plt.gca().xaxis.set_major_formatter(DateFormatter(\"%d.%m\"))\n",
    "    plt.grid()\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(ycol)\n",
    "    plt.plot(data[date], data[ycol], 'b+', label='data')\n",
    "    plt.plot(test[date], test[ycol], 'g+', label='data (test)')\n",
    "    plt.plot(predictdate, predictions, 'r-', label='fit: {}: {}'.format(func.__name__, params_estimate))\n",
    "    plt.title(\"{} {} fit to {}\".format(fname, ycol, func.__name__))\n",
    "    \n",
    "    plt.legend() \n",
    "    plt.savefig(f'{output_folder}/{fname}_{ycol}_{func.__name__}_{seed}.png')\n",
    "    # plt.show() \n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "    for i, group in enumerate(pipeline.groups):\n",
    "        # x = prediction_times = predictx\n",
    "        draws = pipeline.draws[group].copy()\n",
    "        # draws = data_translator(\n",
    "        #     data=draws,\n",
    "        #     input_space=pipeline.predict_space,\n",
    "        #     output_space=pipeline.predict_space\n",
    "        # )\n",
    "        mean_fit = pipeline.mean_predictions[group].copy() # predictions\n",
    "        # mean_fit = data_translator(\n",
    "        #     data=mean_fit,\n",
    "        #     input_space=pipeline.predict_space,\n",
    "        #     output_space=pipeline.predict_space\n",
    "        # )\n",
    "        \n",
    "        mean = draws.mean(axis=0)\n",
    "\n",
    "        # uncertainty\n",
    "        lower = np.quantile(draws, axis=0, q=0.025)\n",
    "        upper = np.quantile(draws, axis=0, q=0.975)\n",
    "\n",
    "    return mean_fit, lower, mean, upper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "predictions = {}\n",
    "for ycol, func in ycols.items():\n",
    "    \n",
    "    data.loc[:,'covs']            = n_data * [ 1.0 ]\n",
    "    data.loc[:,'deaths_normalized']            = data['deaths']/data['deaths'].max()\n",
    "\n",
    "    param_names  = [ 'alpha', 'beta',       'p'     ]\n",
    "    covs = ['covs', 'covs', 'covs']\n",
    "    link_fun     = [ identity_fun, exp_fun, exp_fun ]\n",
    "    var_link_fun = link_fun\n",
    "\n",
    "    # # think this could work with more death data:\n",
    "    # link_fun     = [ identity_fun, identity_fun, exp_fun ]\n",
    "    # covs = ['covs', 'deaths_normalized', 'covs']\n",
    "    \n",
    "    #\n",
    "    pipeline = BasicModel(\n",
    "        all_data=data, #: (pd.DataFrame) of *all* the data that will go into this modeling pipeline\n",
    "        col_t=xcol, #: (str) name of the column with time\n",
    "        col_group=groupcol, #: (str) name of the column with the group in it\n",
    "        col_obs=ycol, #: (str) the name of the column with observations for fitting the model\n",
    "        col_obs_compare=ycol, #TODO: (str) the name of the column that will be used for predictive validity comparison\n",
    "        all_cov_names=covs, #TODO: List[str] list of name(s) of covariate(s). Not the same as the covariate specifications\n",
    "            # that are required by CurveModel in order of parameters. You should exclude intercept from this list.\n",
    "        fun=func, #: (callable) the space to fit in, one of curvefit.functions\n",
    "        predict_space=func, #TODO confirm: (callable) the space to do predictive validity in, one of curvefit.functions\n",
    "        obs_se_func=None, #TODO if we wanna specify: (optional) function to get observation standard error from col_t\n",
    "        # predict_group='all', #: (str) which group to make predictions for\n",
    "        fit_dict={ # TODO: add priors here\n",
    "            'fe_init': params_true / 3.0,\n",
    "            'smart_initialize': pargs['smart_init'],\n",
    "        }, #: keyword arguments to CurveModel.fit_params()\n",
    "        basic_model_dict= { #: additional keyword arguments to the CurveModel class\n",
    "            'col_obs_se': None,#(str) of observation standard error\n",
    "            # 'col_covs': num_params*[covs],#TODO: List[str] list of names of covariates to put on the parameters\n",
    "            'col_covs': [[cov] for cov in covs],#TODO: List[str] list of names of covariates to put on the parameters\n",
    "            'param_names': param_names,#(list{str}):\n",
    "                # Names of the parameters in the specific functional form.\n",
    "            'link_fun': link_fun,#(list{function}):\n",
    "                # List of link functions for each parameter.\n",
    "            'var_link_fun': var_link_fun,#(list{function}):\n",
    "                # List of link functions for the variables including fixed effects\n",
    "                # and random effects.\n",
    "        },\n",
    "    )\n",
    "\n",
    "    predictdate = pd.to_datetime(pd.Series([timedelta(days=x)+data[date].iloc[0] for x in range(-daysback,daysforward)]))\n",
    "    print(f'predictdate {predictdate}')\n",
    "    predictions[ycol] = fit_predict_plot(pipeline, xcol, ycol, data, test, func, predictdate, pargs=pargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = \"MH\"\n",
    "mp = pipeline\n",
    "prediction_times = np.array([x+1 for x in range(-daysback,daysforward)])\n",
    "print(len(prediction_times))\n",
    "print(len(predictdate))\n",
    "\n",
    "data = mp.all_data.loc[mp.all_data[mp.col_group] == group].copy()\n",
    "max_t = int(np.round(data[mp.col_t].max()))\n",
    "print (f'max_t {max_t}')\n",
    "num_obs = data.loc[~data[mp.col_obs_compare].isnull()][mp.col_group].count()\n",
    "print (f'num_obs {num_obs}')\n",
    "\n",
    "predictions = mp.mean_predictions[group]\n",
    "\n",
    "add_noise = prediction_times > max_t\n",
    "no_noise = prediction_times <= max_t\n",
    "print(np.sum(add_noise), np.sum(no_noise))\n",
    "forecast_out_times = prediction_times[add_noise] - max_t\n",
    "print (f'forecast_out_times {forecast_out_times}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frmo pv\n",
    "all_residuals = pipeline.pv.all_residuals\n",
    "print(all_residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after running residual model fit\n",
    "smoothed = pipeline.forecaster.residual_model.smoothed\n",
    "print(smoothed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "data_dict = {'far_out': forecast_out_times, 'num_data': np.array([num_obs])}\n",
    "rows = itertools.product(*data_dict.values())\n",
    "new_data = pd.DataFrame.from_records(rows, columns=data_dict.keys())\n",
    "new_data['data_index'] = new_data['far_out'] + new_data['num_data']\n",
    "\n",
    "new_data['residual_mean'] = 0\n",
    "rmdata = pipeline.forecaster.residual_model.data\n",
    "# print(\"new_data\")\n",
    "# print(new_data)\n",
    "rmdata=new_data\n",
    "# new_data['residual_std'] = self.residual_model.predict(df=new_data)\n",
    "\n",
    "\n",
    "\n",
    "index = rmdata.index\n",
    "rmdata = rmdata.merge(smoothed, on=[\"far_out\", \"num_data\"], how='left', sort=False)\n",
    "rmdata = rmdata.iloc[index]\n",
    "print(\"rmdata\")\n",
    "print(rmdata)\n",
    "outcome = 'residual_std'\n",
    "\n",
    "# corner_value = smoothed[smoothed['num_data'] == smoothed['num_data'].max()]['residual_std'].mean()\n",
    "# no mean here\n",
    "# it is a representation of having a lot of data, which is always true in the future\n",
    "corner_value = smoothed[smoothed['num_data'] == smoothed['num_data'].max()]['residual_std'].mean()\n",
    "counter = 0\n",
    "for i, row in rmdata.iterrows():\n",
    "    if np.isnan(row[outcome]):\n",
    "        # row['num_data'] = num_rows_training\n",
    "        # smoothed['num_data'].max() = num_rows_training - 1\n",
    "        df_sub = smoothed[smoothed['far_out'] == row['far_out']].copy() \n",
    "        if df_sub.empty:\n",
    "            new_val = corner_value\n",
    "        else:\n",
    "            counter += 1\n",
    "#             max_far_out = df_sub[~df_sub['residual_std'].isnull()]['far_out'].max()\n",
    "            new_val = np.nanmean(df_sub['residual_std']) # why mean 1 value\n",
    "            \n",
    "        rmdata.at[i, 'residual_std'] = new_val\n",
    "print(counter, corner_value)\n",
    "print(rmdata[:30])\n",
    "print(rmdata[30:33])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual model predicts, then uses this to calc error/std\n",
    "residuals = pipeline.forecaster.predict(\n",
    "            far_out=forecast_out_times, num_data=np.array([num_obs]))\n",
    "print(residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvgroup = pipeline.pv.pv_groups[group]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_matrix = pvgroup.prediction_matrix\n",
    "compare_observations = pvgroup.compare_observations\n",
    "theta=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_matrix = (prediction_matrix - compare_observations) / (prediction_matrix ** theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexes 14-22 are all 0s in prediction_matrix\n",
    "\n",
    "remove_rows = (data[pipeline.col_t] > 15) & (data['state'] == group)\n",
    "dfr = data[~remove_rows].copy()\n",
    "            \n",
    "\n",
    "pvgroup.models[14].fit(df=dfr, group=group)\n",
    "pvgroup.models[14].predict(times=np.unique(pipeline.all_data[pipeline.col_t].values), predict_space=func, predict_group=group)\n",
    "pvgroup.models[14].mod.params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
