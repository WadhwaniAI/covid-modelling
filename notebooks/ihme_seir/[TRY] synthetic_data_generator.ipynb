{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEIR with synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiments:\n",
    "\n",
    "- Replace ground truth data from various compartments with synthetic data\n",
    "- Time intervals\n",
    "- Functions: log_erf, log_derf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "from datetime import timedelta, datetime\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "from utils.enums import Columns\n",
    "from utils.util import read_config, train_test_split\n",
    "from utils.data import cities\n",
    "from utils.loss import Loss_Calculator\n",
    "\n",
    "from data.dataloader import Covid19IndiaLoader\n",
    "from data.processing import get_dataframes_cached, get_data\n",
    "\n",
    "from main.ihme.fitting import single_cycle, create_output_folder\n",
    "from main.seir.fitting import get_regional_data, data_setup, run_cycle\n",
    "\n",
    "from models.seir.seir_testing import SEIR_Testing\n",
    "from main.seir.forecast import get_forecast\n",
    "\n",
    "from viz.forecast import plot_forecast_agnostic\n",
    "from viz.fit import plot_fit\n",
    "from viz.synthetic_data import plot_fit_uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to create custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_custom_dataset(df_actual, df_synthetic, use_actual=True, use_synthetic=True,\n",
    "                       start_date=None, allowance=5, split1=15, split2=15, split3=15):\n",
    "\n",
    "    if start_date is None:\n",
    "        start_date = df_actual['date'].min()\n",
    "    else:\n",
    "        start_date = pd.to_datetime(start_date, dayfirst=False)\n",
    "        \n",
    "    threshold = start_date - timedelta(days=1)\n",
    "\n",
    "    _, df_actual = train_test_split(df_actual, threshold)\n",
    "    _, df_synthetic = train_test_split(df_synthetic, threshold)\n",
    "        \n",
    "    test_size = split3\n",
    "    end_of_train = start_date + timedelta(allowance+split1+split2 - 1)\n",
    "    \n",
    "    properties = {\n",
    "        \"allowance_before_train\": allowance,\n",
    "        \"total_length\": allowance+split1+split2+split3,\n",
    "        \"train_length (s1+s2)\": split1+split2,\n",
    "        \"test_length (s3)\": test_size,\n",
    "        \"s1_length\": split1,\n",
    "        \"s2_length\": split2,\n",
    "        \"use_synthetic_s2\": (use_actual and use_synthetic)\n",
    "    }\n",
    "    \n",
    "    df_train, df_test = train_test_split(df_actual, end_of_train)\n",
    "\n",
    "    if not use_synthetic:  \n",
    "        pass\n",
    "    elif not use_actual:\n",
    "        df_train, _ = train_test_split(df_synthetic, end_of_train)\n",
    "    elif use_actual and use_synthetic:\n",
    "        end_of_actual = start_date + timedelta(split1 + allowance - 1)\n",
    "        df_train1, _ = train_test_split(df_actual, end_of_actual)\n",
    "        df_train_temp, _ = train_test_split(df_synthetic, end_of_train)\n",
    "        _, df_train2 = train_test_split(df_train_temp, end_of_actual)\n",
    "        df_train = pd.concat([df_train1, df_train2], axis=0)\n",
    "    else:\n",
    "        raise Exception(\"Train and test sets not defined.\")\n",
    "\n",
    "    if len(df_test) > test_size:\n",
    "        df_test = df_test.head(test_size)\n",
    "    else:\n",
    "        raise Exception(\"Test set size {} greater than size available {}.\".format(test_size, len(df_test)))\n",
    "        \n",
    "    df_train.reset_index(inplace=True, drop=True)\n",
    "        \n",
    "    df = pd.concat([df_train, df_test], axis=0)\n",
    "    \n",
    "    return df, df_train, df_test, properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_custom_dataset(dataset, state=None, district=None, compartments=Columns.curve_fit_compartments()):\n",
    "    \"\"\"Format custom dataset according to the format required by the SEIR model. Select/insert required columns.\"\"\"\n",
    "    col_names = [col.name for col in compartments]\n",
    "    dataset = dataset[['date']+col_names]\n",
    "    if state:\n",
    "        dataset.insert(1, \"state\", state)\n",
    "    if district:\n",
    "        dataset.insert(2, \"district\", district)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_custom_dataset_into_dataframes(dataframes, dataset, start_date=None, compartments=None):\n",
    "    \"\"\"Replace original df_district with one or more columns from custom dataset.\"\"\"\n",
    "    if compartments is None:\n",
    "        compartments = Columns.curve_fit_compartments()\n",
    "        col_names = [col.name for col in compartments]\n",
    "    else:\n",
    "        col_names = compartments\n",
    "        \n",
    "    df_district, df_raw = dataframes\n",
    "    \n",
    "    if start_date is None:\n",
    "        start_date = df_district['date'].min()\n",
    "    else:\n",
    "        start_date = pd.to_datetime(start_date, dayfirst=False)\n",
    "    \n",
    "    threshold = start_date - timedelta(days=1)\n",
    "\n",
    "    _, df_district = train_test_split(df_district, threshold)\n",
    "    _, df_raw = train_test_split(df_raw, threshold)\n",
    "    \n",
    "    num_rows = dataset.shape[0]\n",
    "    df_district = df_district.head(num_rows)\n",
    "    for col in col_names:\n",
    "        df_district[col] = dataset[col].values\n",
    "    return (df_district, df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "district = 'Pune'\n",
    "state = 'Maharashtra'\n",
    "disable_tracker = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = Covid19IndiaLoader()\n",
    "dataframes = loader.get_covid19india_api_data()\n",
    "\n",
    "data = get_data(dataframes, state, district, disable_tracker=disable_tracker)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowance = 5 # number of days of actuals to have before s1 starts for rolling average\n",
    "s1 = 10\n",
    "s2 = 5\n",
    "s3 = 7\n",
    "delay = 15 # number of days by which to shift series forward from the first available date of data\n",
    "ihme_val_size = 3\n",
    "seir_c1_train_period = s1\n",
    "seir_c1_val_period = s2\n",
    "seir_c2_train_period = 7\n",
    "seir_c2_val_period = s3\n",
    "smooth_window_ihme = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_properties = {\n",
    "    'allowance': allowance,\n",
    "    's1': s1,\n",
    "    's2': s2,\n",
    "    's3': s3,\n",
    "    'shift': delay,\n",
    "    'ihme_val_size': ihme_val_size,\n",
    "    'seir_c1_train_period': seir_c1_train_period,\n",
    "    'seir_c1_val_period': seir_c1_val_period,\n",
    "    'seir_c2_train_period': seir_c2_train_period,\n",
    "    'seir_c2_val_period': seir_c2_val_period\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = data['date'].min() + timedelta(delay)\n",
    "ihme_start_date = start_date + timedelta(allowance)\n",
    "dataset_length = s1 + s2 \n",
    "smooth_jump = True if district == \"Mumbai\" else False\n",
    "replace = ['hospitalised', 'total_infected', 'deceased', 'recovered'] # buckets for which synthetic data is used\n",
    "which_compartments = ['hospitalised', 'total_infected', 'deceased', 'recovered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "folder = f'{district}/{str(now)}'\n",
    "output_folder = create_output_folder(f'synth/{folder}/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IHME model (I1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist, st, area_names = cities[district.lower()]\n",
    "config, model_params = read_config('../../scripts/ihme/config/default.yaml')\n",
    "config['start_date'] = ihme_start_date\n",
    "config['dataset_length'] = dataset_length\n",
    "config['disable_tracker'] = disable_tracker\n",
    "config['max_evals'] = 1\n",
    "config['test_size'] = s2\n",
    "config['val_size'] = ihme_val_size\n",
    "config['min_days'] = 7\n",
    "config['n_days_optimize'] = False\n",
    "config['smooth'] = smooth_window_ihme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ihme_res = single_cycle(dist, st, area_names, model_params, **config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "ihme_df_train, ihme_df_val = ihme_res['df_train'], ihme_res['df_val']\n",
    "ihme_df_train_nora, ihme_df_val_nora = ihme_res['df_train_nora'], ihme_res['df_val_nora']\n",
    "ihme_df_true = ihme_res['df_district']\n",
    "ihme_df_pred = ihme_res['df_prediction']\n",
    "\n",
    "makesum = copy.deepcopy(ihme_df_pred)\n",
    "makesum['total_infected'] = ihme_df_pred['recovered'] + ihme_df_pred['deceased'] + ihme_df_pred['hospitalised']\n",
    "\n",
    "plot_fit(\n",
    "    makesum.reset_index(), ihme_df_train, ihme_df_val, ihme_df_train_nora, ihme_df_val_nora, \n",
    "    s1, st, dist, which_compartments=[c.name for c in Columns.curve_fit_compartments()],\n",
    "    description = 'Train and test',\n",
    "    savepath=os.path.join(output_folder, 'ihme.png'))\n",
    "\n",
    "plot_forecast_agnostic(ihme_df_true, makesum.reset_index(), model_name='IHME M1', \n",
    "                       dist=dist, state=st, filename=os.path.join(output_folder, 'ihme-forecast.png'))\n",
    "\n",
    "\n",
    "for plot_col in Columns.which_compartments():\n",
    "    plot_fit_uncertainty(makesum.reset_index(), ihme_df_train, ihme_df_val, ihme_df_train_nora, ihme_df_val_nora, \n",
    "                         s1, s2, st, dist, draws=ihme_res['draws'],\n",
    "                         which_compartments=[plot_col.name], \n",
    "                         description = 'Train and test',\n",
    "                         savepath=os.path.join(output_folder, f'ihme_{plot_col.name}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ihme_res['df_prediction']['total_infected'] = makesum['total_infected']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draws = ihme_res['draws']\n",
    "average_uncertainty_s2 = dict()\n",
    "for compartment in [c.name for c in Columns.which_compartments()]:\n",
    "    draws_compartment = draws[compartment]['draws']\n",
    "    draws_compartment_s2 = draws_compartment[:, s1:s1+s2]\n",
    "    average_uncertainty_s2[compartment] = np.mean(draws_compartment_s2[1] - draws_compartment_s2[0])\n",
    "uncertainty = pd.DataFrame.from_dict(average_uncertainty_s2, orient='index', columns=['average s2 uncertainty'])\n",
    "ihme_res['df_loss'] = pd.concat([ihme_res['df_loss'],uncertainty],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i1 = ihme_res['df_loss'].T[['hospitalised', 'deceased', 'recovered', 'total_infected']]\n",
    "i1.to_csv(output_folder+\"ihme.csv\")\n",
    "ihme_res['df_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEIR model (C1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_input = get_regional_data(dataframes, st, dist, (not disable_tracker), None, None, granular_data=False, \n",
    "                             smooth_jump=smooth_jump, smoothing_length=33, smoothing_method='weighted', t_recov=14,\n",
    "                             return_extra=False, which_compartments=which_compartments)\n",
    "c1_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=SEIR_Testing\n",
    "variable_param_ranges=None\n",
    "data_from_tracker=False\n",
    "granular_data=False\n",
    "filename=None\n",
    "data_format='new'\n",
    "train_period=seir_c1_train_period\n",
    "val_period=seir_c1_val_period\n",
    "num_evals=1500\n",
    "N=1e7\n",
    "initialisation='intermediate'\n",
    "which_compartments=['hospitalised', 'total_infected', 'deceased', 'recovered']\n",
    "smooth_jump=smooth_jump\n",
    "smoothing_length=33\n",
    "smoothing_method='weighted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_df_district, c1_df_raw = c1_input\n",
    "c1_df_district = c1_df_district.head(delay+train_period+val_period+allowance)\n",
    "\n",
    "print(c1_df_district)\n",
    "\n",
    "predictions_dict_c1 = dict()\n",
    "\n",
    "observed_dataframes = data_setup(c1_df_district, c1_df_raw, val_period)\n",
    "\n",
    "print('train\\n', observed_dataframes['df_train'])\n",
    "print('val\\n', observed_dataframes['df_val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dict_c1['m1'] = run_cycle(\n",
    "        st, dist, observed_dataframes, \n",
    "        model=model, variable_param_ranges=variable_param_ranges,\n",
    "        data_from_tracker=data_from_tracker, train_period=train_period, \n",
    "        which_compartments=which_compartments, N=N,\n",
    "        num_evals=num_evals, initialisation=initialisation\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dict_c1['m1']['ax'].savefig(output_folder+'seir-c1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = predictions_dict_c1['m1']['df_loss'].T[['hospitalised', 'deceased', 'recovered', 'total_infected']]\n",
    "c1.to_csv(output_folder+\"seir.csv\")\n",
    "predictions_dict_c1['m1']['df_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create custom datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1, train1, test1, prop1 = get_custom_dataset(ihme_res['df_district_nora'], ihme_res['df_prediction'], \n",
    "                                               use_synthetic=False, start_date=start_date,\n",
    "                                               allowance=allowance, split1=s1, split2=s2, split3=s3)\n",
    "train1 = format_custom_dataset(train1, st, dist)\n",
    "df1 = format_custom_dataset(df1, st, dist)\n",
    "train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2, train2, test2, prop2 = get_custom_dataset(ihme_res['df_district_nora'], ihme_res['df_prediction'],\n",
    "                                               start_date=start_date,\n",
    "                                               allowance=allowance, split1=s1, split2=s2, split3=s3)\n",
    "train2 = format_custom_dataset(train2, st, dist)\n",
    "df2 = format_custom_dataset(df2, st, dist)\n",
    "train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3, train3, test3, prop3 = get_custom_dataset(ihme_res['df_district_nora'], predictions_dict_c1['m1']['df_prediction'], \n",
    "                                               start_date=start_date,\n",
    "                                               allowance=allowance, split1=s1, split2=s2, split3=s3)\n",
    "train3 = format_custom_dataset(train3, st, dist)\n",
    "df3 = format_custom_dataset(df3, st, dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save custom datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(output_folder+\"Dataset1.csv\")\n",
    "df2.to_csv(output_folder+\"Dataset2.csv\")\n",
    "df3.to_csv(output_folder+\"Dataset3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEIR Model using custom datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = get_regional_data(dataframes, st, dist, (not disable_tracker), None, None, granular_data=False, \n",
    "                             smooth_jump=smooth_jump, smoothing_length=33, smoothing_method='weighted', t_recov=14,\n",
    "                             return_extra=False, which_compartments=which_compartments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = insert_custom_dataset_into_dataframes(input_df, df1, \n",
    "                                                start_date=start_date, compartments=replace)\n",
    "input_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_2 = insert_custom_dataset_into_dataframes(input_df, df2, \n",
    "                                                start_date=start_date, compartments=replace)\n",
    "input_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_3 = insert_custom_dataset_into_dataframes(input_df, df3, \n",
    "                                                start_date=start_date, compartments=replace)\n",
    "input_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=SEIR_Testing\n",
    "variable_param_ranges=None\n",
    "data_from_tracker=(not disable_tracker)\n",
    "granular_data=False\n",
    "filename=None\n",
    "data_format='new'\n",
    "train_period=seir_c2_train_period\n",
    "val_period=seir_c2_val_period\n",
    "num_evals=1500\n",
    "N=1e7\n",
    "initialisation='intermediate'\n",
    "which_compartments=['hospitalised', 'total_infected', 'deceased', 'recovered']\n",
    "smooth_jump=smooth_jump\n",
    "smoothing_length=33\n",
    "smoothing_method='weighted'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1 - Ground truth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dict_1 = dict()\n",
    "\n",
    "observed_dataframes = data_setup(input_1[0], input_1[1], val_period)\n",
    "\n",
    "print('train\\n', observed_dataframes['df_train'])\n",
    "print('val\\n', observed_dataframes['df_val'])\n",
    "\n",
    "predictions_dict_1['m1'] = run_cycle(\n",
    "        st, dist, observed_dataframes, \n",
    "        model=model, variable_param_ranges=variable_param_ranges,\n",
    "        data_from_tracker=data_from_tracker, train_period=train_period, \n",
    "        which_compartments=which_compartments, N=N,\n",
    "        num_evals=num_evals, initialisation=initialisation\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = predictions_dict_1['m1']['df_loss'].T[['hospitalised', 'deceased', 'recovered', 'total_infected']]\n",
    "t1['exp'] = 1\n",
    "predictions_dict_1['m1']['df_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dict_1['m1']['ax'].savefig(output_folder+'seir-exp1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2 - Ground truth + IHME forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dict_2 = dict()\n",
    "\n",
    "observed_dataframes = data_setup(input_2[0], input_2[1], val_period)\n",
    "\n",
    "print('train\\n', observed_dataframes['df_train']) \n",
    "print('val\\n', observed_dataframes['df_val'])\n",
    "\n",
    "predictions_dict_2['m1'] = run_cycle(\n",
    "        st, dist, observed_dataframes, \n",
    "        model=model, variable_param_ranges=variable_param_ranges,\n",
    "        data_from_tracker=data_from_tracker, train_period=train_period, \n",
    "        which_compartments=which_compartments, N=N,\n",
    "        num_evals=num_evals, initialisation=initialisation\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = predictions_dict_2['m1']['df_loss'].T[['hospitalised', 'deceased', 'recovered', 'total_infected']]\n",
    "t2['exp'] = 2\n",
    "predictions_dict_2['m1']['df_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dict_2['m1']['ax'].savefig(output_folder+'seir-exp2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3 - Ground truth + SEIR forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dict_3 = dict()\n",
    "\n",
    "observed_dataframes = data_setup(input_3[0], input_3[1], val_period)\n",
    "\n",
    "print('train\\n', observed_dataframes['df_train']) \n",
    "print('val\\n', observed_dataframes['df_val'])\n",
    "\n",
    "predictions_dict_3['m1'] = run_cycle(\n",
    "        st, dist, observed_dataframes, \n",
    "        model=model, variable_param_ranges=variable_param_ranges,\n",
    "        data_from_tracker=data_from_tracker, train_period=train_period, \n",
    "        which_compartments=which_compartments, N=N,\n",
    "        num_evals=num_evals, initialisation=initialisation\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = predictions_dict_3['m1']['df_loss'].T[['hospitalised', 'deceased', 'recovered', 'total_infected']]\n",
    "t3['exp'] = 3\n",
    "predictions_dict_3['m1']['df_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dict_3['m1']['ax'].savefig(output_folder+'seir-exp3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.concat([t1, t2, t3], axis=0)\n",
    "t.index.name=\"index\"\n",
    "t.sort_values(by='index', inplace=True)\n",
    "t.to_csv(output_folder+\"/exp_\"+\"_\".join(replace)+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dicts = [predictions_dict_1, predictions_dict_2, predictions_dict_3]\n",
    "train_dicts = [train1, train2, train3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_across_datasets(exp_no, fig, ax, df_true, df_prediction, df_train, dist, state, \n",
    "                         s1_start, train_start, s2_start, test_start, s3_end, graph_start, graph_end,\n",
    "                         log_scale=False, filename=None,\n",
    "                         model_name='M2', which_compartments=Columns.which_compartments()):\n",
    "    for col in Columns.which_compartments():\n",
    "        if col in which_compartments:\n",
    "            ax.plot(df_true['date'], df_true[col.name],\n",
    "                '-o', color=col.color, label=f'{col.label} (Observed)')\n",
    "            ax.plot(df_train['date'], df_train[col.name],\n",
    "                'x', color=col.color, label=f'{col.label} (Train data)')\n",
    "            ax.plot(df_prediction[\"date\"], df_prediction[col.name],\n",
    "                    '-', color=col.color, label=f'{col.label} ({model_name} Forecast)')\n",
    "            \n",
    "            s1_start = pd.to_datetime(s1_start)\n",
    "            train_start = pd.to_datetime(train_start)\n",
    "            s2_start = pd.to_datetime(s2_start)\n",
    "            test_start = pd.to_datetime(test_start)\n",
    "            s3_end = pd.to_datetime(s3_end)\n",
    "            graph_start = pd.to_datetime(graph_start)\n",
    "            graph_end = pd.to_datetime(graph_end)\n",
    "\n",
    "            line_height = plt.ylim()[1]\n",
    "            ax.plot([train_start, train_start], [0,line_height], '--', color='black', label='Train starts')\n",
    "            ax.plot([test_start, test_start], [0,line_height], '--', color='black', label='Test starts')\n",
    "\n",
    "            ax.xaxis.set_major_locator(mdates.DayLocator(interval=7))\n",
    "            ax.xaxis.set_minor_locator(mdates.DayLocator(interval=1))\n",
    "            ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "            \n",
    "            ax.axvspan(s1_start, s2_start, alpha=0.1, color='red')\n",
    "            ax.axvspan(s2_start, test_start, alpha=0.1, color='yellow')\n",
    "            ax.axvspan(test_start, s3_end, alpha=0.1, color='green')\n",
    "    \n",
    "            ax.legend(loc=\"upper left\")\n",
    "            ax.tick_params(labelrotation=45)\n",
    "            ax.grid()\n",
    "            \n",
    "            ax.set_xlim(graph_start, graph_end)\n",
    "            \n",
    "            ax.set_xlabel('Time', fontsize=10)\n",
    "            ax.set_ylabel('No of People', fontsize=10)\n",
    "            \n",
    "            if exp_no == 0: data = 'Using ground truth data'\n",
    "            if exp_no == 1: data = 'Using data from IHME forecast'\n",
    "            if exp_no == 2: data = 'Using data from SEIR forecast'\n",
    "            \n",
    "            ax.title.set_text('Forecast - {} - {}'.format(dist, data))\n",
    "        \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_start = (ihme_start_date - timedelta(allowance+1)).strftime(\"%m-%d-%Y\")\n",
    "s1_start = (ihme_start_date).strftime(\"%m-%d-%Y\")\n",
    "s2_start = (ihme_start_date + timedelta(s1)).strftime(\"%m-%d-%Y\")\n",
    "test_start = (ihme_start_date + timedelta(s1+s2)).strftime(\"%m-%d-%Y\")\n",
    "train_start = (ihme_start_date + timedelta(s1+s2-seir_c2_train_period)).strftime(\"%m-%d-%Y\")\n",
    "s3_end = (ihme_start_date + timedelta(s1+s2+s3)).strftime(\"%m-%d-%Y\")\n",
    "graph_end = (ihme_start_date + timedelta(s1+s2+s3+1)).strftime(\"%m-%d-%Y\")\n",
    "\n",
    "for col in range(4):\n",
    "    fig, ax = plt.subplots(3, sharex=True, sharey=True, figsize=(15, 15))\n",
    "    for row in range(len(ax)):\n",
    "        ax[row] = plot_across_datasets(row, fig, ax[row], input_df[0].iloc[delay:,:].head(allowance+s1+s2+s3), \n",
    "                                       predictions_dicts[row]['m1']['df_prediction'], \n",
    "                                       predictions_dicts[row]['m1']['df_district'], \n",
    "                                       district, state, s1_start, train_start, s2_start, test_start, s3_end, graph_start, graph_end,\n",
    "                                       model_name='', which_compartments=[Columns.which_compartments()[col]],)\n",
    "        \n",
    "        filename = output_folder+Columns.which_compartments()[col].name\n",
    "        fig.savefig(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dict = {\n",
    "    'compartments_replaced': replace,\n",
    "    'dataset_properties': {\n",
    "        'exp1': prop1,\n",
    "        'exp2': prop2,\n",
    "        'exp3': prop3\n",
    "    },\n",
    "    'series_properties': series_properties\n",
    "}\n",
    "\n",
    "with open(output_folder+'params.json', 'w') as outfile:\n",
    "    json.dump(params_dict, outfile, indent=4)\n",
    "    \n",
    "config['start_date'] = config['start_date'].strftime(\"%Y-%m-%d\")\n",
    "with open(output_folder+'ihme-config.json', 'w') as outfile:\n",
    "    json.dump(config, outfile, indent=4)\n",
    "    \n",
    "with open(output_folder+'ihme-model-params.json', 'w') as outfile:\n",
    "    json.dump(repr(model_params), outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
