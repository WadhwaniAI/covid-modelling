{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import multiprocessing as mp\n",
    "import arviz as az\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 18})\n",
    "from joblib import delayed, Parallel\n",
    "from collections import defaultdict, OrderedDict\n",
    "import pymc3 as pm\n",
    "from pymc3.ode import DifferentialEquation\n",
    "from utils.generic import init_params\n",
    "from main.seir.optimiser import Optimiser\n",
    "from utils.loss import Loss_Calculator\n",
    "from models.seir.seir_testing import SEIR_Testing\n",
    "from data.processing import get_district_time_series\n",
    "from data.dataloader import Covid19IndiaLoader\n",
    "from theano.ifelse import ifelse\n",
    "from theano import tensor as T\n",
    "from theano import tensor as T, function, printing\n",
    "from theano import function\n",
    "from theano import config\n",
    "config.compute_test_value='ignore'\n",
    "config.gcc.cxxflags = \"-Wno-c++11-narrowing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.ode.DifferentialEquation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load covid19 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = Covid19IndiaLoader()\n",
    "dataframes = loader.get_covid19india_api_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = [('Delhi', ''), ('Karnataka', 'Bengaluru Urban'), ('Maharashtra', 'Mumbai'), ('Maharashtra', 'Pune'), ('Gujarat', 'Ahmedabad'), ('Rajasthan', 'Jaipur')]\n",
    "state, district = regions[2]\n",
    "df_district = get_district_time_series(dataframes, state=state, district=district, use_dataframe='districts_daily')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train-val splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_district.iloc[:-7, :]\n",
    "df_val = df_district.iloc[-7:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('df_train.csv')\n",
    "df_val.to_csv('df_val.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_calculator = Loss_Calculator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set priors for parameters of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## assuming uniform priors, following dictionary contains the ranges\n",
    "prior_ranges = OrderedDict()\n",
    "prior_ranges['lockdown_R0'] = (1, 2)\n",
    "prior_ranges['T_inc'] = (1, 5) \n",
    "prior_ranges['T_inf'] = (1, 4) \n",
    "prior_ranges['T_recov_severe'] = (5, 60)\n",
    "prior_ranges['P_severe'] = (0.3, 0.99)\n",
    "prior_ranges['P_fatal'] = (0, 0.3)\n",
    "prior_ranges['sigma'] = (0, 1)\n",
    "\n",
    "with pm.Model() as model:\n",
    "    prior_R0 = pm.Uniform(\"R0\", lower =1, upper=3)\n",
    "    prior_T_inc = pm.Uniform(\"T_inc\", 1, 5)\n",
    "    prior_T_inf = pm.Uniform(\"T_inf\", 1, 4)\n",
    "    prior_T_recov_severe = pm.Uniform(\"T_recov_severe \", 9, 20)\n",
    "    prior_P_severe = pm.Uniform(\"P_severe\", 0.3, 0.99)\n",
    "    prior_intervention_amount = pm.Uniform(\"intervention_amount\", 0.3, 1)\n",
    "\n",
    "\n",
    "def param_init(prior_ranges):\n",
    "    theta = defaultdict()\n",
    "    for key in prior_ranges:\n",
    "        theta[key] = np.random.uniform(prior_ranges[key][0], prior_ranges[key][1])\n",
    "        \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposal function to sample theta_new given theta_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal_sigmas = OrderedDict()\n",
    "for key in prior_ranges:\n",
    "    proposal_sigmas[key] = 0.025 * (prior_ranges[key][1] - prior_ranges[key][0])\n",
    "\n",
    "def proposal(theta_old, proposal_sigmas):\n",
    "    theta_new = np.random.normal(loc=[*theta_old.values()], scale=[*proposal_sigmas.values()])\n",
    "    return dict(zip(theta_old.keys(), theta_new))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Likelihood and Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(theta, df_train, fit_days=10):\n",
    "    if (np.array([*theta.values()]) < 0).any():\n",
    "        return -np.inf\n",
    "    optimiser = Optimiser()\n",
    "    default_params = optimiser.init_default_params(df_train)\n",
    "    df_prediction = optimiser.solve(theta, default_params, df_train)\n",
    "    pred = np.array(df_prediction['total_infected'])[-fit_days:]\n",
    "    true = np.array(df_train['total_infected'])[-fit_days:]\n",
    "    sigma = theta['sigma']\n",
    "    N = len(true)\n",
    "    ll = - (N * np.log(np.sqrt(2*np.pi) * sigma)) - (np.sum(((true - pred) ** 2) / (2 * sigma ** 2)))\n",
    "    return ll\n",
    "\n",
    "def log_prior(theta):\n",
    "    if (np.array([*theta.values()]) < 0).any():\n",
    "        prior = 0\n",
    "    else:\n",
    "        prior = 1\n",
    "    return np.log(prior)\n",
    "\n",
    "def in_valid_range(key, value):\n",
    "    return (value <= prior_ranges[key][1]) and (value >= prior_ranges[key][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acceptance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accept(theta_old, theta_new, df_train):    \n",
    "    x_new = log_likelihood(theta_new, df_train) + log_prior(theta_new)\n",
    "    x_old = log_likelihood(theta_old, df_train) + log_prior(theta_old)\n",
    "    \n",
    "    if (x_new) > (x_old):\n",
    "        return True\n",
    "    else:\n",
    "        x = np.random.uniform(0, 1)\n",
    "        return (x < np.exp(x_new - x_old))\n",
    "    \n",
    "def anneal_accept(iter):\n",
    "    prob = 1 - np.exp(-(1/(iter + 1e-10)))\n",
    "    x = np.random.uniform(0, 1)\n",
    "    return (x < prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metropolis loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metropolis(prior_ranges, proposal_sigmas, df_train, iter=1000):\n",
    "    theta = param_init(prior_ranges)\n",
    "    accepted = [theta]\n",
    "    rejected = list()\n",
    "    \n",
    "    for i in tqdm(range(iter)):\n",
    "        theta_new = proposal(theta, proposal_sigmas)\n",
    "        if accept(theta, theta_new, df_train):\n",
    "            theta = theta_new\n",
    "        else:\n",
    "            rejected.append(theta_new)\n",
    "        accepted.append(theta)\n",
    "    \n",
    "    return accepted, rejected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Interval calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PI(pred_dfs, date, key, multiplier=1.96):\n",
    "    pred_samples = list()\n",
    "    for df in pred_dfs:\n",
    "        pred_samples.append(df.loc[date, key])\n",
    "        \n",
    "    mu = np.array(pred_samples).mean()\n",
    "    sigma = np.array(pred_samples).std()\n",
    "    low = mu - multiplier*sigma\n",
    "    high = mu + multiplier*sigma\n",
    "    return mu, low, high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run multiple chains in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SEIR_Test_pymc3(SEIR_Testing):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def get_derivative(self, y, t, p):\n",
    "        # Init state variables\n",
    "        #for i, _ in enumerate(y):\n",
    "        #for i in range(11):\n",
    "        #    y[i] = ifelse(T.lt(y[i], 0), y[i], np.float64(0))\n",
    "        #    y[i] = max(y[i], 0)\n",
    "        zero = T.cast(0.0, 'float64')\n",
    "        for i in range(11):\n",
    "            T.set_subtensor(y[i], ifelse(T.gt(y[i], zero), y[i], zero))\n",
    "        # Init time parameters and probabilities\n",
    "        for key in self.vanilla_params:\n",
    "            setattr(self, key, self.vanilla_params[key])\n",
    "        for key in self.testing_params:\n",
    "            suffix = '_D' if key in self.vanilla_params else ''\n",
    "            setattr(self, key + suffix, self.testing_params[key])\n",
    "            \n",
    "        \n",
    "        ## Set up variables using `y` and `p`\n",
    "        \n",
    "        S = y[0]\n",
    "        E = y[1]\n",
    "        I = y[2]\n",
    "        D_E = y[3]\n",
    "        D_I = y[4]\n",
    "        R_mild = y[5]\n",
    "        R_severe_home = y[6]\n",
    "        R_severe_hosp = y[7]\n",
    "        R_fatal = y[8]\n",
    "        C = y[9]\n",
    "        D = y[10]\n",
    "        \n",
    "        # p\n",
    "    \n",
    "        self.R0 = p[0]\n",
    "        self.T_inc = p[1]\n",
    "        self.T_inf = p[2]\n",
    "        self.T_recov_severe = p[3]\n",
    "        self.P_severe = p[4]\n",
    "        self.intervention_amount = p[5]\n",
    "        \n",
    "        #Define variables\n",
    "        \n",
    "  \n",
    "        if self.post_lockdown_R0 == None:\n",
    "            self.post_lockdown_R0 = self.lockdown_R0\n",
    "\n",
    "        self.P_mild = 1 - self.P_severe - self.P_fatal\n",
    "\n",
    "        # define testing related parameters\n",
    "        self.T_inf_detected = self.T_inf\n",
    "        self.T_inc_detected = self.T_inc\n",
    "\n",
    "        self.P_mild_detected = self.P_mild\n",
    "        self.P_severe_detected = self.P_severe\n",
    "        self.P_fatal_detected = self.P_fatal\n",
    "        #self.T_trans_D = self.T_trans\n",
    "  \n",
    "        #TODO incorporate lockdown R0 code\n",
    "        #T.set_subtensor(self.R0, ifelse(T.gt(t, self.lockdown_removal_day), self.R0 , self.post_lockdown_R0))\n",
    "        # Modelling the behaviour lockdown\n",
    "        #elif t >= self.lockdown_day:\n",
    "        #    self.R0 = self.lockdown_R0\n",
    "        #T.set_subtensor(self.R0, ifelse(T.gt(t, self.lockdown_day), self.R0, self.lockdown_R0))\n",
    "        # Modelling the behaviour pre-lockdown\n",
    "        #else:\n",
    "        #    self.R0 = self.pre_lockdown_R0\n",
    "        #T.set_subtensor(self.R0, ifelse(T.gt(y[i], zero), self.R0, self.pre_lockdown_R0))\n",
    "        self.T_trans = self.T_inf/self.R0\n",
    "        self.T_trans_D = self.T_inf_D/self.R0\n",
    "        \n",
    "       \n",
    "        # Write differential equations\n",
    "        dS = - I * S / (self.T_trans) - (self.q / self.T_trans_D) * (S * D_I) # # S\n",
    "        #dS = - y[2] * y[0]*p[0]/p[2]  - self.q*p[2] * (y[0] * y[4])\n",
    "        dE = I * S / (self.T_trans) + (self.q / self.T_trans_D) * (S * D_I) - (E/ self.T_inc) - (self.theta_E * self.psi_E * E) # E\n",
    "        dI = E / self.T_inc - I / self.T_inf - (self.theta_I * self.psi_I * I) # I\n",
    "        dD_E = (self.theta_E * self.psi_E * E) - (1 / self.T_inc_D) * D_E# D_E\n",
    "        dD_I = (self.theta_I * self.psi_I * I) + (1 / self.T_inc_D) * D_E - (1 / self.T_inf_D) * D_I # D_I \n",
    "        dR_mild = (1/self.T_inf)*(self.P_mild*I) + (1/self.T_inf_D)*(self.P_mild_D*D_I) - R_mild/self.T_recov_mild  # R_mild\n",
    "        dR_severe_home = (1/self.T_inf)*(self.P_severe*I) + (1/self.T_inf_D)*(self.P_severe_D*D_I) - R_severe_home/self.T_hosp  # R_severe_home\n",
    "        dR_severe_hosp = R_severe_home/self.T_hosp - R_severe_hosp/self.T_recov_severe# R_severe_hosp\n",
    "        dR_fatal = (1/self.T_inf)*(self.P_fatal*I) + (1/self.T_inf_D)*(self.P_fatal_D*D_I) - R_fatal/self.T_recov_fatal # R_fatal\n",
    "        dC = R_mild/self.T_recov_mild + R_severe_hosp/self.T_recov_severe # C\n",
    "        dD = R_fatal/self.T_recov_fatal # D\n",
    "\n",
    "        return [dS, dE, dI, dD_E, dD_I, dR_mild, dR_severe_home, dR_severe_hosp, dR_fatal, dC, dD]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = T.scalar('x')\n",
    "z = T.scalar('z')\n",
    "xplus = ifelse(T.lt(x, z), x, z)\n",
    "xplus.eval({x:1,z:0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed = df_train['total_infected'][:num_steps]\n",
    "\n",
    "SEIR_Test_obj = SEIR_Test_pymc3()\n",
    "num_patients = SEIR_Test_obj.__dict__['vanilla_params']['N']\n",
    "init_vals = list(SEIR_Test_obj.__dict__['state_init_values'].values())\n",
    "num_steps = 5\n",
    "num_states = 11\n",
    "burn_in = 500\n",
    "total_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sir_model = DifferentialEquation(\n",
    "    func=SEIR_Test_obj.get_derivative,\n",
    "    times=np.arange(0, num_steps, 1),\n",
    "    n_states= num_states,\n",
    "    n_theta=6,\n",
    "    t0 = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    R0 = pm.Uniform(\"R0\", lower =1, upper=3)#(1.6, 3)\n",
    "    T_inc = pm.Uniform(\"T_inc\", 1, 5)#(3, 4)\n",
    "    T_inf = pm.Uniform(\"T_inf\", 1, 4)#(3, 4)\n",
    "    T_recov_severe = pm.Uniform(\"T_recov_severe \", 9, 20)\n",
    "    P_severe = pm.Uniform(\"P_severe\", 0.3, 0.99)\n",
    "    intervention_amount = pm.Uniform(\"intervention_amount\", 0.3, 1)\n",
    "    \n",
    "    ode_solution = sir_model(y0=init_vals , theta=[R0, T_inc, T_inf, T_recov_severe, P_severe, intervention_amount])\n",
    "    # The ode_solution has a shape of (n_times, n_states)\n",
    "    \n",
    "    hospitalised = ode_solution[:,6] + ode_solution[:,7] + ode_solution[:,8]\n",
    "    recovered = ode_solution[:,9]\n",
    "    deceased = ode_solution[:,10]\n",
    "    total_infected = hospitalised + recovered + deceased\n",
    "    total_infected = total_infected * num_patients \n",
    "    sigma = pm.HalfNormal('sigma',\n",
    "                          sigma=observed.std(),\n",
    "                          shape=num_steps)\n",
    "    Y = pm.Normal('Y', mu = total_infected, sigma=sigma, observed=observed)\n",
    "    \n",
    "    prior = pm.sample_prior_predictive()\n",
    "    trace = pm.sample(total_steps, tune=burn_in , target_accept=0.9, cores=mp.cpu_count())\n",
    "    posterior_predictive = pm.sample_posterior_predictive(trace)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printing_op = printing.Print('vector', attrs = [ 'shape' ])\n",
    "shape_infected = printing_op(total_infected)\n",
    "shape_sigma = printing_op(sigma)\n",
    "printval = printing.Print('vector')\n",
    "printval(total_infected* num_patients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    data = az.from_pymc3(trace=trace, prior=prior, posterior_predictive=posterior_predictive)\n",
    "    az.plot_posterior(data,round_to=2, credible_interval=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.forestplot(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_runs = trace[burn_in:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the samples and intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(): \n",
    "    data_split = df_district.copy()\n",
    "    optimiser = Optimiser()\n",
    "    default_params = optimiser.init_default_params(data_split)\n",
    "    \n",
    "    #combined_acc = list()\n",
    "    #for k, run in enumerate(mcmc):\n",
    "    #    burn_in = int(len(run) / 2)\n",
    "    #    combined_acc += run[0][burn_in:]\n",
    "\n",
    "    n_samples = 1000\n",
    "    sample_indices = np.random.uniform(0, len(final_runs), n_samples)\n",
    "\n",
    "    pred_dfs = list()\n",
    "    for i in tqdm(sample_indices):\n",
    "        pred_dfs.append(optimiser.solve(final_runs[int(i)], default_params, data_split))\n",
    "\n",
    "    for df in pred_dfs:\n",
    "        df.set_index('date', inplace=True)\n",
    "\n",
    "    result = pred_dfs[0].copy()\n",
    "    for col in result.columns:\n",
    "        result[\"{}_low\".format(col)] = ''\n",
    "        result[\"{}_high\".format(col)] = ''\n",
    "\n",
    "    for date in tqdm(pred_dfs[0].index):\n",
    "        for key in pred_dfs[0]:\n",
    "            result.loc[date, key], result.loc[date, \"{}_low\".format(key)], result.loc[date, \"{}_high\".format(key)] = get_PI(pred_dfs, date, key)\n",
    "\n",
    "    data_split.set_index(\"date\", inplace=True)\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.plot(data_split['total_infected'].tolist(), c='g', label='Actual')\n",
    "    plt.plot(result['total_infected'].tolist(), c='r', label='Estimated')\n",
    "    plt.plot(result['total_infected_low'].tolist(), c='r', linestyle='dashdot')\n",
    "    plt.plot(result['total_infected_high'].tolist(), c='r', linestyle='dashdot')\n",
    "    plt.axvline(x=len(df_train), c='b', linestyle='dashed')\n",
    "    plt.xlabel(\"Day\")\n",
    "    plt.ylabel(\"Total infected\")\n",
    "    plt.legend()\n",
    "    plt.title(\"95% confidence intervals for {}, {}\".format(district, state))\n",
    "    \n",
    "    plt.savefig('./mcmc_confidence_intervals_{}_{}.png'.format(district, state))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize all runs separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 50))\n",
    "    \n",
    "for k, run in enumerate(mcmc):\n",
    "    data_split = df_district.copy()\n",
    "    optimiser = Optimiser()\n",
    "    default_params = optimiser.init_default_params(data_split)\n",
    "    \n",
    "    acc, rej = run[0], run[1]\n",
    "    df_samples = pd.DataFrame(acc)\n",
    "    \n",
    "    plt.subplot(len(mcmc), 3, 3*k + 1)\n",
    "    for param in df_samples.columns:\n",
    "        plt.plot(list(range(len(df_samples[param]))), df_samples[param], label=param)\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Accepted samples from run {}\".format(k+1))\n",
    "    \n",
    "    rej_samples = pd.DataFrame(rej)\n",
    "    \n",
    "    plt.subplot(len(mcmc), 3, 3*k + 2)\n",
    "    for param in rej_samples.columns:\n",
    "        plt.scatter(list(range(len(rej_samples[param]))), rej_samples[param], label=param, s=2)\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Rejected samples from run {}\".format(k+1))\n",
    "    \n",
    "    burn_in = int(len(acc) / 2)\n",
    "    n_samples = 1000\n",
    "    posterior_samples = acc[burn_in:]\n",
    "    sample_indices = np.random.uniform(0, len(posterior_samples), n_samples)\n",
    "\n",
    "    pred_dfs = list()\n",
    "    for i in tqdm(sample_indices):\n",
    "        pred_dfs.append(optimiser.solve(posterior_samples[int(i)], default_params, data_split))\n",
    "        \n",
    "    for df in pred_dfs:\n",
    "        df.set_index('date', inplace=True)\n",
    "        \n",
    "    result = pred_dfs[0].copy()\n",
    "    for col in result.columns:\n",
    "        result[\"{}_low\".format(col)] = ''\n",
    "        result[\"{}_high\".format(col)] = ''\n",
    "        \n",
    "    for date in tqdm(pred_dfs[0].index):\n",
    "        for key in pred_dfs[0]:\n",
    "            result.loc[date, key], result.loc[date, \"{}_low\".format(key)], result.loc[date, \"{}_high\".format(key)] = get_PI(pred_dfs, date, key)\n",
    "            \n",
    "    data_split.set_index(\"date\", inplace=True)\n",
    "\n",
    "    plt.subplot(len(mcmc), 3, 3*k + 3)\n",
    "    plt.plot(data_split['total_infected'].tolist(), c='g', label='Actual')\n",
    "    plt.plot(result['total_infected'].tolist(), c='r', label='Estimated')\n",
    "    plt.plot(result['total_infected_low'].tolist(), c='r', linestyle='dashdot')\n",
    "    plt.plot(result['total_infected_high'].tolist(), c='r', linestyle='dashdot')\n",
    "    plt.axvline(x=len(df_train), c='b', linestyle='dashed')\n",
    "    plt.xlabel(\"Day\")\n",
    "    plt.ylabel(\"Total infected\")\n",
    "    plt.legend()\n",
    "    plt.title(\"95% CIs for {}, {} from run {}\".format(district, state, k+1))\n",
    "    \n",
    "plt.savefig(\"./mcmc_runs_{}_{}.png\".format(district, state))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Checking validity with Gelman-Rubin statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Section 4.2 [here](http://www.columbia.edu/~mh2078/MachineLearningORFE/MCMC_Bayes.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate(dict_list):\n",
    "    accumulator = defaultdict(int)\n",
    "    for elt in dict_list:\n",
    "        for key in elt:\n",
    "            accumulator[key]+=elt[key]\n",
    "    return accumulator\n",
    "\n",
    "def divide(dictvar, num):\n",
    "    return {key:dictvar[key]/num for key in dictvar}\n",
    "\n",
    "            \n",
    "def avg_sum_chain(chain):\n",
    "    chain_sums_avg = accumulate(chain)\n",
    "    return divide(chain_sums_avg, len(chain))\n",
    "\n",
    "def avg_sum_multiple_chains(chain_sums_avg):\n",
    "    multiple_chain_sums_avg = accumulate(chain_sums_avg)\n",
    "    return divide(multiple_chain_sums_avg, len(chain_sums_avg))\n",
    "\n",
    "def compute_B(multiple_chain_sums_avg, chain_sums_avg, n, m):\n",
    "    B = defaultdict(int)\n",
    "    for elt in chain_sums_avg:\n",
    "        for key in elt:\n",
    "            B[key] += np.square(elt[key] - multiple_chain_sums_avg[key])\n",
    "    return divide(B, (m-1)/n)\n",
    "\n",
    "def compute_W(split_chains, chain_sums_avg, n, m):\n",
    "    s = []\n",
    "    for j in range(m):\n",
    "        s_j_sq = defaultdict(int)\n",
    "        chain = split_chains[j]\n",
    "        chain_sum_avg_j = chain_sums_avg[j]\n",
    "        for i in range(n):\n",
    "            chain_elt = chain[i]\n",
    "            for key in chain_elt:\n",
    "                s_j_sq[key] += np.square(chain_elt[key] - chain_sum_avg_j [key])\n",
    "        s_j_sq = divide(s_j_sq, n - 1)\n",
    "        s.append(s_j_sq)\n",
    "    return (divide (accumulate(s),m))\n",
    "\n",
    "def divide_dict(d1, d2):\n",
    "    accumulator = defaultdict(int)\n",
    "    for key in d1:\n",
    "        accumulator[key] = d1[key]/d2[key]\n",
    "    return accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burn_in = int(len(mcmc[0][0]) / 2)\n",
    "chains = [mcmc_chain[0] for mcmc_chain in mcmc]\n",
    "burn_in = int(len(chains[0]) / 2)\n",
    "sampled_chains = [chain[:burn_in] for chain in chains]\n",
    "split_chains = [sampled_chain[int(burn_in/2):] for sampled_chain in sampled_chains] \\\n",
    "            + [sampled_chain[:int(burn_in/2)] for sampled_chain in sampled_chains]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_sums_avg = []\n",
    "for chain in split_chains:\n",
    "    chain_sums_avg.append(avg_sum_chain(chain))\n",
    "multiple_chain_sums_avg = avg_sum_multiple_chains(chain_sums_avg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_chain_sums_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(split_chains)\n",
    "n = len(split_chains[0])\n",
    "W =  compute_W(split_chains, chain_sums_avg, n, m)\n",
    "B =  compute_B(multiple_chain_sums_avg, chain_sums_avg, n, m)\n",
    "var_hat = accumulate([divide(W, n/(n-1)), divide(B, n) ])\n",
    "R_hat_sq = divide_dict(var_hat, W)\n",
    "R_hat = {key:np.sqrt(value) for key, value in R_hat_sq.items()}\n",
    "neff = divide_dict(var_hat, B)\n",
    "neff = {key: m*n*value for key, value in neff.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_hat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (covid)",
   "language": "python",
   "name": "covid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
