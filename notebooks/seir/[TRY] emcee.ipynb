{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import emcee\n",
    "from multiprocessing import Pool\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "import corner\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 18})\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "from utils.generic import init_params\n",
    "from main.seir.optimiser import Optimiser\n",
    "from models.seir.seir_testing import SEIR_Testing\n",
    "from data.processing import get_district_time_series\n",
    "from data.dataloader import get_covid19india_api_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load covid19 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = get_covid19india_api_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_district = get_district_time_series(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Make splits\n",
    "df_train = df_district"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Calculation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calc_rmse(y_pred, y_true, log=True):\n",
    "    if log:\n",
    "        y_true = np.log(y_true)\n",
    "        y_pred = np.log(y_pred)\n",
    "    loss = np.sqrt(np.mean((y_true - y_pred)**2))\n",
    "    return loss\n",
    "\n",
    "def _calc_mape(y_pred, y_true):\n",
    "    y_pred = y_pred[y_true > 0]\n",
    "    y_true = y_true[y_true > 0]\n",
    "\n",
    "    ape = np.abs((y_true - y_pred + 0) / y_true) *  100\n",
    "    loss = np.mean(ape)\n",
    "    return loss\n",
    "\n",
    "def calc_loss_dict(states_time_matrix, df, method='rmse', rmse_log=False):\n",
    "    pred_hospitalisations = states_time_matrix[6] + states_time_matrix[7] + states_time_matrix[8]\n",
    "    pred_recoveries = states_time_matrix[9]\n",
    "    pred_fatalities = states_time_matrix[10]\n",
    "    pred_infectious_unknown = states_time_matrix[2] + states_time_matrix[4]\n",
    "    pred_total_cases = pred_hospitalisations + pred_recoveries + pred_fatalities\n",
    "    \n",
    "    if method == 'rmse':\n",
    "        if rmse_log:\n",
    "            calculate = lambda x, y : _calc_rmse(x, y)\n",
    "        else:\n",
    "            calculate = lambda x, y : _calc_rmse(x, y, log=False)\n",
    "    \n",
    "    if method == 'mape':\n",
    "            calculate = lambda x, y : _calc_mape(x, y)\n",
    "    \n",
    "    losses = {}\n",
    "#     losses['hospitalised'] = calculate(pred_hospitalisations, df['Hospitalised'])\n",
    "#     losses['recovered'] = calculate(pred_recoveries, df['Recovered'])\n",
    "#     losses['fatalities'] = calculate(pred_fatalities, df['Fatalities'])\n",
    "#     losses['active_infections'] = calculate(pred_infectious_unknown, df['Active Infections (Unknown)'])\n",
    "    losses['total'] = calculate(pred_total_cases, df['total_infected'])\n",
    "    \n",
    "    return losses\n",
    "\n",
    "def calc_loss(states_time_matrix, df, method='rmse', rmse_log=False):\n",
    "    losses = calc_loss_dict(states_time_matrix, df, method, rmse_log)\n",
    "#     loss = losses['hospitalised'] + losses['recovered'] + losses['total'] + losses['active_infections']\n",
    "    loss = losses['total']\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize params and state values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_params, testing_params, state_init_values = init_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_init_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set priors for parameters of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## assuming uniform priors, following dictionary contains the ranges\n",
    "prior_ranges = OrderedDict()\n",
    "prior_ranges['R0'] = (1, 3)#(1.6, 3)\n",
    "prior_ranges['T_inc'] = (1, 5) #(4, 5)\n",
    "prior_ranges['T_inf'] = (1, 4) #(3, 4)\n",
    "prior_ranges['T_recov_severe'] = (9, 20)\n",
    "prior_ranges['P_severe'] = (0.3, 0.99)\n",
    "prior_ranges['intervention_amount'] = (0.3, 1)\n",
    "prior_ranges['c_sigma'] = (0.001, 5)\n",
    "\n",
    "def param_init():\n",
    "    theta = defaultdict()\n",
    "    for key in prior_ranges:\n",
    "        theta[key] = np.random.uniform(prior_ranges[key][0], prior_ranges[key][1])\n",
    "        \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposal function to sample theta_new given theta_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal_sigmas = OrderedDict()\n",
    "for key in prior_ranges:\n",
    "    proposal_sigmas[key] = 1#0.025 * (prior_ranges[key][1] - prior_ranges[key][0])\n",
    "\n",
    "def proposal(theta_old):\n",
    "    theta_new = np.random.normal(loc=[*theta_old.values()], scale=[*proposal_sigmas.values()])\n",
    "    return dict(zip(theta_old.keys(), theta_new))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Likelihood and Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(OrderedDict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "def log_likelihood(theta):\n",
    "    if (np.array([*theta.values()]) < 0).any():\n",
    "        return -np.inf\n",
    "    #alpha = 0.01\n",
    "    optimiser = Optimiser()\n",
    "    default_params = optimiser.init_default_params(df_train)\n",
    "    df_prediction = optimiser.solve(theta, default_params, df_train)\n",
    "    pred = np.array(df_prediction['total_infected'])\n",
    "    true = np.array(df_train['total_infected'])\n",
    "    #sigma = alpha * true.std()\n",
    "    sigma = theta['c_sigma']\n",
    "    N = len(true)\n",
    "    ll = - (N * np.log(np.sqrt(2*np.pi) * sigma)) - (np.sum(((true - pred) ** 2) / (2 * sigma ** 2)))\n",
    "    return ll\n",
    "\n",
    "def log_prior(theta):\n",
    "#     prior = 1\n",
    "#     for key in prior_ranges:\n",
    "#         if in_valid_range(key, theta[key]):\n",
    "#             prior *= 1 / (prior_ranges[key][1] - prior_ranges[key][0])\n",
    "#         else:\n",
    "#             prior = 0\n",
    "#             break\n",
    "    if (np.array([*theta.values()]) < 0).any():\n",
    "        prior = 0\n",
    "    else:\n",
    "        prior = 1\n",
    "    \n",
    "    return np.log(prior)\n",
    "\n",
    "def in_valid_range(key, value):\n",
    "    return (value <= prior_ranges[key][1]) and (value >= prior_ranges[key][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acceptance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1\n",
    "key_list = ['R0','T_inc','T_inf','T_recov_severe','P_severe','intervention_amount', 'c_sigma']\n",
    "\n",
    "def convert_to_dict(theta):\n",
    "    if type(theta) != dict:\n",
    "        theta_vals = theta.copy()\n",
    "        theta = dict()\n",
    "        for key_index in range(len(key_list)):\n",
    "            theta[key_list[key_index]] = theta_vals[key_index]\n",
    "    return theta\n",
    "\n",
    "def log_probability(theta):\n",
    "    theta = convert_to_dict(theta)\n",
    "    return log_likelihood(theta) + log_prior(theta)\n",
    "\n",
    "def accept(theta_old, theta_new, boltzmann = False):  \n",
    "    x_new = log_probability(theta_new)\n",
    "    x_old = log_probability(theta_old)\n",
    "    \n",
    "    if (x_new) > (x_old):\n",
    "        return True\n",
    "    else:\n",
    "        x = np.random.uniform(0, 1)\n",
    "        return (x < np.exp(x_new - x_old))\n",
    "    \n",
    "def anneal_accept(iter):\n",
    "    prob = 1 - np.exp(-(1/(iter + 1e-10)))\n",
    "    x = np.random.uniform(0, 1)\n",
    "    return (x < prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimum_params = { 'R0': 2.1039262514239443, 'T_inc': 4.29031222687138,\n",
    "                  'T_inf': 3.0377562096514046, 'T_recov_severe': 9.594716552601186,\n",
    "                  'P_severe': 0.961325014139492, 'intervention_amount': 0.4101211254804955,\n",
    "                    'c_sigma' : 1} # Sigma unknown, need to fit to likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimum_params.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metropolis loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metropolis(iter=1000):\n",
    "    theta = param_init()\n",
    "    accepted = [theta]\n",
    "    rejected = list()\n",
    "    \n",
    "    for i in tqdm(range(iter)):\n",
    "        theta_new = proposal(theta)\n",
    "        if anneal_accept(i):\n",
    "            theta = theta_new\n",
    "        else:\n",
    "            if accept(theta, theta_new):\n",
    "                theta = theta_new\n",
    "            else:\n",
    "                rejected.append(theta_new)\n",
    "        accepted.append(theta)\n",
    "    \n",
    "    return accepted, rejected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "proposal_function â€“ The proposal function. It should take 2 arguments: a numpy-compatible random number generator and a (K, ndim) list of coordinate vectors. This function should return the proposed position and the log-ratio of the proposal probabilities (lnð‘ž(ð‘¥;ð‘¥â€²)âˆ’lnð‘ž(ð‘¥â€²;ð‘¥) where ð‘¥â€² is the proposed coordinate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def proposal_emcee(rng, theta_list):\n",
    "    theta = theta_list[-1]\n",
    "    theta_new = proposal(theta)\n",
    "    if anneal_accept(i):\n",
    "        theta = theta_new\n",
    "    return theta, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_param = []\n",
    "for key in prior_ranges:\n",
    "    init_param.append(optimum_params[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Emcee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwalkers = 30\n",
    "nsteps = 20000\n",
    "ndim = len(init_param)\n",
    "pos = init_param + 1e-2 * np.random.randn(nwalkers, ndim)\n",
    "filename = \"emcee.h5\"\n",
    "backend = emcee.backends.HDFBackend(filename)\n",
    "backend.reset(nwalkers, ndim)\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_probability,\n",
    "                                #moves=[(emcee.moves.DEMove(), 0.8), (emcee.moves.DESnookerMove(), 0.2),],\n",
    "                                backend = backend,\n",
    "                                   a=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"start = time.time()\n",
    "sampler.run_mcmc(pos, nsteps, progress=True);\n",
    "end = time.time()\n",
    "serial_time = end - start\n",
    "\n",
    "   \n",
    "    start = time.time()\n",
    "    sampler.run_mcmc(pos, nsteps, progress=True);\n",
    "    end = time.time()\n",
    "    multi_time = end - start\n",
    "    print(\"Multiprocessing took {0:.1f} seconds\".format(multi_time))\n",
    "    print(\"{0:.1f} times faster than serial\".format(serial_time / multi_time))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run EMcee sampler\n",
    "sampler.run_mcmc(pos, nsteps, progress=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acc, rej = metropolis(iter=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_samples = pd.DataFrame(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sampler.get_chain()\n",
    "#samples[:,sampler.acceptance_fraction > 0.1,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.acceptance_fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Time series of parameter values for multiple chains\n",
    "\n",
    "fig, axes = plt.subplots(ndim, figsize=(10, 20), sharex=True)\n",
    "samples = sampler.get_chain()\n",
    "#samples = samples[:,sampler.acceptance_fraction > 0.1,:]\n",
    "labels = key_list\n",
    "for i in range(ndim):\n",
    "    ax = axes[i]\n",
    "    ax.plot(samples[:, :, i], \"k\", alpha=0.3)\n",
    "    ax.set_xlim(0, len(samples))\n",
    "    ax.set_ylabel(labels[i])\n",
    "    ax.yaxis.set_label_coords(-0.1, 0.5)\n",
    "\n",
    "axes[-1].set_xlabel(\"step number\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get autocorrelation time of the chain\n",
    "tau = sampler.get_autocorr_time()\n",
    "print(tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_samples = sampler.get_chain(discard=200, thin=20, flat=True)#[:,sampler.acceptance_fraction > 0.1,:]\n",
    "#num_samples, num_chains, num_params = flat_samples.shape\n",
    "#flat_samples = flat_samples.reshape((num_samples*num_chains, num_params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_samples[:,0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View corner plot to check pairwise correlations\n",
    "corner.corner(flat_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(flat_samples[:,0], bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use samples to estimate confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dfs = list()\n",
    "optimiser = Optimiser()\n",
    "default_params = optimiser.init_default_params(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_indices = np.random.randint(len(flat_samples), size=1000)\n",
    "posterior_samples = flat_samples\n",
    "for i in tqdm(sample_indices):\n",
    "    pred_dfs.append(optimiser.solve( convert_to_dict(posterior_samples[int(i)]), default_params, df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in pred_dfs:\n",
    "    df.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pred_dfs[0].copy()\n",
    "for col in result.columns:\n",
    "    result[\"{}_low\".format(col)] = ''\n",
    "    result[\"{}_high\".format(col)] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PI(date, key, multiplier=1.96):\n",
    "    scaling_factor = 1\n",
    "    pred_samples = list()\n",
    "    for df in pred_dfs:\n",
    "        pred_samples.append(df.loc[date, key])\n",
    "    mu = np.array(pred_samples).mean()\n",
    "    sigma =  scaling_factor*np.array(pred_samples).std()\n",
    "    low = mu - multiplier*sigma\n",
    "    high = mu + multiplier*sigma\n",
    "    return mu, low, high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " pred_dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in tqdm(pred_dfs[0].index):\n",
    "    for key in pred_dfs[0]:\n",
    "        result.loc[date, key], result.loc[date, \"{}_low\".format(key)], result.loc[date, \"{}_high\".format(key)] = get_PI(date, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(df_train['total_infected'], c='g', label='Actual')\n",
    "plt.plot(result['total_infected'].tolist(), c='r', label='Estimated')\n",
    "plt.plot(result['total_infected_low'].tolist(), c='r', linestyle='dashdot')\n",
    "plt.plot(result['total_infected_high'].tolist(), c='r', linestyle='dashdot')\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"Total infected\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
