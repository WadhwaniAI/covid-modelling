{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import copy\n",
    "import os\n",
    "import yaml\n",
    "from datetime import timedelta\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "from data.processing.processing import generate_simulated_data\n",
    "from data.dataloader import SimulatedDataLoader\n",
    "\n",
    "from main.seir.fitting import single_fitting_cycle\n",
    "from utils.generic.config import read_config\n",
    "from utils.fitting.loss import Loss_Calculator\n",
    "from utils.fitting.smooth_jump import smooth_big_jump\n",
    "\n",
    "from viz.data import plot_data\n",
    "from utils.generic.enums.columns import *\n",
    "from viz.utils import setup_plt, axis_formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_filename = 'simulate_data.yaml'\n",
    "config = read_config(config_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_smoothing_experiment(config, smoothing_method='weighted-mag'):\n",
    "    # Load synthetic data config file\n",
    "    config_file = config['fitting']['data']['dataloading_params']['config_file']\n",
    "    with open(os.path.join(\"../../configs/simulated_data/\", config_file)) as configfile:\n",
    "        sim_config = yaml.load(configfile, Loader=yaml.SafeLoader)\n",
    "    # Generate synthetic data\n",
    "    loader = SimulatedDataLoader()\n",
    "    res = loader.load_data(**sim_config)\n",
    "    df = res['data_frame']\n",
    "\n",
    "    # Perform smoothing\n",
    "    dl_params = copy.deepcopy(config['fitting']['data']['dataloading_params'])\n",
    "    dl_params['simulate_spike'] = True\n",
    "    # Randomly generate frac_to_report and smoothing length params\n",
    "    start_date = dl_params['simulate_spike_params']['start_date']\n",
    "    spike_days = np.random.randint(low=14, high=28)\n",
    "    # Simulate Spike\n",
    "    dl_params['simulate_spike_params']['end_date'] = start_date + timedelta(days=spike_days)\n",
    "    print('spike generation params', dl_params['simulate_spike_params'])\n",
    "    df_spike = loader.simulate_spike(df=df, **dl_params['simulate_spike_params'])\n",
    "    \n",
    "    # Create smoothing params on the basis of start and end date of spikes\n",
    "    comp = dl_params['simulate_spike_params']['comp']\n",
    "    edate = dl_params['simulate_spike_params']['end_date']\n",
    "    sdate = dl_params['simulate_spike_params']['start_date']\n",
    "    smooth_params = {}\n",
    "    smooth_params[edate - timedelta(days=1)] = [sdate, comp, 'active', False, smoothing_method]\n",
    "\n",
    "    # Perform smoothing\n",
    "    print('smoothing params', smooth_params)\n",
    "    df_spike_smooth, _ = smooth_big_jump(df_spike, smooth_params)\n",
    "\n",
    "    # Compare df_spike_smooth and df\n",
    "    processed_vals = df_spike_smooth.loc[(df_spike_smooth['date'].dt.date >= sdate) & \n",
    "                                         (df_spike_smooth['date'].dt.date <= edate), ['active', 'recovered']].to_numpy()\n",
    "    true_vals = df.loc[(df['date'].dt.date >= sdate) & (df['date'].dt.date <= edate), ['active', 'recovered']].to_numpy()\n",
    "    return (np.mean(np.abs(processed_vals - true_vals)/true_vals, axis=0)*100).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(5000):\n",
    "    results.append(perform_smoothing_experiment(config, 'uniform'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(results).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = config['fitting']['data']['dataloading_params']['config_file']\n",
    "with open(os.path.join(\"../../configs/simulated_data/\", config_file)) as configfile:\n",
    "    sim_config = yaml.load(configfile, Loader=yaml.SafeLoader)\n",
    "# Generate synthetic data\n",
    "loader = SimulatedDataLoader()\n",
    "res = loader.load_data(**sim_config)\n",
    "df = res['data_frame']\n",
    "\n",
    "# Perform smoothing\n",
    "dl_params = copy.deepcopy(config['fitting']['data']['dataloading_params'])\n",
    "dl_params['simulate_spike'] = True\n",
    "# Randomly generate frac_to_report and smoothing length params\n",
    "start_date = dl_params['simulate_spike_params']['start_date']\n",
    "spike_days = np.random.randint(low=14, high=28)\n",
    "# Simulate Spike\n",
    "dl_params['simulate_spike_params']['end_date'] = start_date + timedelta(days=spike_days)\n",
    "print('spike generation params', dl_params['simulate_spike_params'])\n",
    "df_spike = loader.simulate_spike(df=df, **dl_params['simulate_spike_params'])\n",
    "\n",
    "# Create smoothing params on the basis of start and end date of spikes\n",
    "comp = dl_params['simulate_spike_params']['comp']\n",
    "edate = dl_params['simulate_spike_params']['end_date']\n",
    "sdate = dl_params['simulate_spike_params']['start_date']\n",
    "smooth_params = {}\n",
    "smooth_params[edate - timedelta(days=1)] = [sdate, comp, 'active', False, 'weighted-mag']\n",
    "\n",
    "# Perform smoothing\n",
    "print('smoothing params', smooth_params)\n",
    "df_spike_smooth, _ = smooth_big_jump(df_spike, smooth_params)\n",
    "\n",
    "# Compare df_spike_smooth and df\n",
    "processed_vals = df_spike_smooth.loc[(df_spike_smooth['date'].dt.date >= sdate) & \n",
    "                                     (df_spike_smooth['date'].dt.date <= edate), ['active', 'recovered']].to_numpy()\n",
    "true_vals = df.loc[(df['date'].dt.date >= sdate) & (df['date'].dt.date <= edate), ['active', 'recovered']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_data(df_spike_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "## for Palatino and other serif fonts use:\n",
    "plt.rcParams.update({\n",
    "    'text.usetex': True,\n",
    "    'font.size': 20,\n",
    "    'font.family': 'Palatino',\n",
    " })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "for comp in ['active', 'total', 'recovered', 'deceased']:\n",
    "    compartment = Columns.from_name(comp)\n",
    "    ax.plot(df[compartments['date'][0].name].to_numpy(), df[compartment.name].to_numpy(),\n",
    "            '--', color=compartment.color, label='Simulated Data, Unspiked ({})'.format(compartment.label))\n",
    "    ax.plot(df_spike[compartments['date'][0].name].to_numpy(), df_spike[compartment.name].to_numpy(),\n",
    "            '-o', color=compartment.color, label='Simulated Data, Spiked ({})'.format(compartment.label), ms=5)\n",
    "    ax.plot(df_spike_smooth[compartments['date'][0].name].to_numpy(), df_spike_smooth[compartment.name].to_numpy(),\n",
    "            '-', color=compartment.color, label='Smoothed Data ({})'.format(compartment.label))\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], ls='--', color='black', label='Simulated Data, Unspiked'),\n",
    "    Line2D([0], [0], ls='-', marker='o', ms=5, color='black', label='Simulated Data, Spiked'),\n",
    "    Line2D([0], [0], ls='-', color='black', label='Smoothed Data')\n",
    "]\n",
    "first_legend = ax.legend(handles=legend_elements, loc='upper left')\n",
    "ax.add_artist(first_legend)\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], ls='-', color='C0', label=f'Confirmed Cases'),\n",
    "    Line2D([0], [0], ls='-', color='orange', label=f'Active Cases'),\n",
    "    Line2D([0], [0], ls='-', color='green', label=f'Recovered'),\n",
    "    Line2D([0], [0], ls='-', color='red', label=f'Deceased'),\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc=[0.015, 0.64])\n",
    "axis_formatter(ax)\n",
    "ax.set_title('Comparison of smoothing algorithm with ground truth simulated data')\n",
    "plt.tight_layout()\n",
    "fig.savefig(f'../../../paper/plots/smoothing-simulated.pdf', format='pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
